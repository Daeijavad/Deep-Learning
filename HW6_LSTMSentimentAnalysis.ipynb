{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daeijavad/Deep-Learning/blob/main/HW6_LSTMSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6ONTUDH5n95P",
        "outputId": "be42680a-7206-43b1-cb67-c4f0a513dee5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCHl7CnWSlBq"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVYGJ9ApSlBs"
      },
      "source": [
        "Welcome to HomeWork 6  &#128522;&#9996;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh01PD-5SlBu"
      },
      "source": [
        "## Part II : Sentiment Analysis Using RNNs\n",
        "\n",
        "In this part, you will use reccurent neural networks to solve the Sentiment Analysis problem. \n",
        "\n",
        "At first, we load the required packages. Add other packages if you need them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "vSk7u0B2SlBv",
        "outputId": "c65a4ee6-0af8-4f84-d1cf-f91f786e967c"
      },
      "source": [
        "import tensorflow as tf\n",
        "# or other libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Z7qmmQSlBy"
      },
      "source": [
        "Then, we load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EinmbpXkSlBz"
      },
      "source": [
        "sentiment_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/dataset.txt', sep='\\t')\n",
        "sentiment_data.columns =['Class', 'Data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_VioctcSlB1"
      },
      "source": [
        "Here, you can see some of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jlJUPIwISlB2",
        "outputId": "b51ea106-d48c-4ba6-fa94-5a9fd08a33a1"
      },
      "source": [
        "sentiment_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>this was the first clive cussler i've ever rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>i liked the Da Vinci Code a lot.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>i liked the Da Vinci Code a lot.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>that's not even an exaggeration ) and at midni...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Class                                               Data\n",
              "0      1  this was the first clive cussler i've ever rea...\n",
              "1      1                   i liked the Da Vinci Code a lot.\n",
              "2      1                   i liked the Da Vinci Code a lot.\n",
              "3      1  I liked the Da Vinci Code but it ultimatly did...\n",
              "4      1  that's not even an exaggeration ) and at midni..."
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oYQG3RiSlB5"
      },
      "source": [
        "From now implement your model.\n",
        "<br>\n",
        "Please add cells and explain yours developing steps and your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wat5fQ9MmI77"
      },
      "source": [
        "# Pre proccessing\n",
        "\n",
        "> for this part we first make a martix corresponding to data according to the count of each word\n",
        "\n",
        "> then we reshape it for importing to LSTM\n",
        "\n",
        "> at end we split them into train and test (test contains 10% of data)\n",
        "\n",
        "using https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKwW0wN8SlB6"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentiment_data['Data'])\n",
        "dataX = tokenizer.texts_to_matrix(sentiment_data['Data'], mode='count')\n",
        "\n",
        "dataX = np.reshape(dataX, (dataX.shape[0], dataX.shape[1], 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataX, sentiment_data['Class'],shuffle = True, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "EJRti79mnQrY",
        "outputId": "2fbd9ea8-a5ab-427a-f65b-39060074c767"
      },
      "source": [
        "#sample data\n",
        "print(X_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " ...\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9AZOUTNnDQl"
      },
      "source": [
        "# model architecture and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "v7oFVoPk5rEB",
        "outputId": "ac4e79d0-7ec2-4683-c8ae-724e2813fb76"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "QXNgtUZYDmMH",
        "outputId": "3c6bc74f-de82-4684-d343-d9f99fbd7f68"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 256)               264192    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 264,449\n",
            "Trainable params: 264,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "_6EnesZODp9K",
        "outputId": "8fc84384-da6c-4575-b0f5-0b8cfe89ce50"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=3, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 6225 samples, validate on 692 samples\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "6225/6225 [==============================] - 394s 63ms/step - loss: 0.6850 - acc: 0.5653 - val_loss: 0.6817 - val_acc: 0.5997\n",
            "Epoch 2/3\n",
            "6225/6225 [==============================] - 376s 60ms/step - loss: 0.6853 - acc: 0.5667 - val_loss: 0.6765 - val_acc: 0.5997\n",
            "Epoch 3/3\n",
            "6225/6225 [==============================] - 371s 60ms/step - loss: 0.6846 - acc: 0.5667 - val_loss: 0.6776 - val_acc: 0.5997\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f52d80fcef0>"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pHIx1PpihMF"
      },
      "source": [
        "> for having a better performance it's good to use word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bhlAJAbnxkv"
      },
      "source": [
        "# Better Model Using Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMvTLQuNqhXH"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentiment_data['Data'])\n",
        "sequences = tokenizer.texts_to_sequences(sentiment_data['Data'])\n",
        "sequences_same_size = sequence.pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences_same_size, sentiment_data['Class'], shuffle = True, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmmchK3STrpR"
      },
      "source": [
        "> this is list that contains words with the number assinged to each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "8SIq0zQJt151",
        "outputId": "93082e0a-3287-4b7c-b619-80f89b75084b"
      },
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('this', 214), ('was', 1175), ('the', 3224), ('first', 104), ('clive', 1), ('cussler', 1), (\"i've\", 11), ('ever', 96), ('read', 113), ('but', 295), ('even', 27), ('books', 29), ('like', 974), ('relic', 1), ('and', 2153), ('da', 1999), ('vinci', 2000), ('code', 1998), ('were', 96), ('more', 103), ('plausible', 1), ('than', 14), ('i', 4645), ('liked', 101), ('a', 1305), ('lot', 22), ('it', 720), ('ultimatly', 1), (\"didn't\", 8), ('seem', 1), ('to', 808), ('hold', 2), (\"it's\", 180), ('own', 4), (\"that's\", 6), ('not', 198), ('an', 225), ('exaggeration', 1), ('at', 110), ('midnight', 1), ('we', 219), ('went', 115), ('wal', 1), ('mart', 1), ('buy', 2), ('which', 252), ('is', 1518), ('amazing', 16), ('of', 424), ('course', 7), ('loved', 256), ('now', 21), ('want', 254), ('something', 7), ('better', 96), ('different', 4), ('thought', 102), ('great', 95), ('same', 87), ('with', 288), ('kite', 1), ('runner', 1), ('actually', 14), ('good', 127), ('movie', 783), ('pretty', 32), ('book', 135), ('one', 283), ('most', 102), ('beautiful', 128), ('movies', 366), ('ive', 2), ('seen', 94), ('do', 16), ('get', 20), ('me', 213), ('wrong', 7), ('then', 108), ('turn', 2), ('on', 116), ('light', 1), ('radio', 1), ('enjoy', 9), ('my', 462), ('really', 374), ('love', 1543), ('night', 9), ('mind', 1), ('awesome', 1126), ('thing', 12), ('enjoyed', 8), ('very', 19), ('slash', 1), ('race', 1), ('hey', 7), ('also', 118), ('would', 181), ('be', 290), ('disappointed', 1), ('in', 176), ('other', 20), ('8230', 1), ('angels', 5), ('demons', 6), ('yeah', 91), ('interesting', 7), (\"code's\", 2), ('backtory', 1), ('various', 1), ('religious', 3), ('historical', 1), ('figures', 2), ('such', 93), ('times', 7), (\"i'm\", 35), ('scifi', 1), ('girl', 4), ('heart', 3), ('s', 84), ('see', 48), ('crazy', 81), ('ian', 1), ('mckellen', 1), ('old', 2), ('gay', 92), ('husband', 1), ('some', 17), ('people', 183), ('will', 19), ('say', 24), ('joe', 1), (\"you're\", 6), ('being', 169), ('too', 122), ('hard', 84), ('dan', 3), ('brown', 2), ('1', 54), ('well', 100), ('did', 25), ('bridget', 1), ('jones', 1), ('so', 506), ('idea', 4), ('appeals', 1), ('takes', 2), ('chick', 1), ('lit', 3), ('into', 253), ('few', 5), ('arenas', 1), ('that', 713), ('genre', 1), ('has', 12), ('yet', 8), ('explore', 1), ('just', 286), ('by', 103), ('way', 101), ('\\xa0', 3), ('excellent', 86), ('if', 195), ('you', 324), ('as', 473), ('normal', 2), ('novels', 1), ('reading', 181), ('telling', 4), ('again', 88), ('opinion', 85), ('may', 12), ('bit', 6), ('biased', 2), ('because', 608), ('soundtrack', 1), ('quite', 6), ('pleased', 1), ('open', 2), ('mindedness', 1), ('after', 20), ('having', 8), ('much', 452), ('able', 2), ('equal', 2), ('enjoyment', 1), ('seeing', 6), ('how', 197), ('side', 84), ('reads', 1), ('oh', 170), ('miss', 10), ('ripping', 1), ('acoustic', 1), ('music', 4), ('personally', 5), ('neither', 1), ('hate', 578), ('nor', 2), ('looks', 8), ('amazingly', 1), ('fun', 84), ('possibly', 1), ('cold', 2), (\"don't\", 100), ('tell', 9), ('anyone', 85), ('figure', 1), ('out', 269), ('page', 2), ('286', 1), ('seriously', 1), ('omg', 4), ('still', 100), ('awesomest', 2), ('though', 29), ('shit', 10), ('got', 94), ('two', 12), ('finished', 6), ('damn', 4), ('fucking', 174), ('sounds', 2), ('probable', 1), ('half', 6), ('crap', 5), ('fr', 1), ('raises', 1), ('theological', 1), ('questions', 1), ('are', 202), ('absurd', 3), ('aaron', 2), ('\\x92', 1), ('mom', 83), ('knew', 2), ('took', 2), ('tour', 1), ('inside', 1), (\"i'd\", 10), ('go', 23), ('probably', 5), ('next', 3), ('week', 6), ('im', 5), ('sick', 5), ('right', 249), ('must', 5), ('far', 10), ('3333', 1), ('am', 259), ('onto', 1), ('angel', 1), ('wanna', 4), (\"brown's\", 1), ('illustrated', 1), ('edition', 1), ('only', 175), (\"can't\", 90), ('wait', 88), ('for', 241), ('recently', 2), ('their', 5), ('come', 4), ('should', 9), ('all', 36), ('up', 180), ('until', 5), ('end', 5), ('where', 83), ('he', 17), ('kind', 5), ('lost', 6), ('totally', 7), ('ca', 1), (\"n't\", 2), ('its', 16), ('thriller', 1), (\"'\", 87), (\"code'it\", 1), ('think', 202), ('about', 129), ('things', 93), ('from', 101), ('angle', 1), ('friggin', 1), ('30', 1), ('pages', 3), ('left', 166), ('undercover', 1), ('3', 272), ('outta', 1), ('three', 10), ('besides', 3), ('drowining', 1), ('myself', 3), ('already', 13), ('saw', 106), ('know', 276), ('lynn', 1), ('jon', 1), ('theaters', 2), ('everyone', 9), ('luau', 1), ('gasp', 1), ('protests', 1), ('against', 2), ('make', 84), ('sad', 12), ('many', 3), ('who', 183), ('share', 1), ('religion', 2), ('idiots', 1), ('dating', 1), ('sister', 1), ('our', 6), ('time', 100), ('together', 2), ('absolutely', 101), (\"doesn't\", 6), ('mean', 7), ('jesus', 4), ('admired', 1), ('bachelor', 1), ('beyond', 1), ('tome', 1), ('unauthorized', 1), ('deals', 1), ('novel', 5), ('what', 96), ('inspired', 1), ('both', 89), ('x', 8), ('men', 7), ('last', 92), ('stand', 81), ('day', 6), ('they', 23), ('came', 3), ('talking', 7), ('agree', 3), ('ridiculous', 1), ('panties', 1), ('bunch', 1), ('heresies', 1), ('blashpemies', 1), ('whatever', 2), ('hell', 3), ('else', 7), ('allegedly', 1), ('contains', 1), ('lah', 2), ('watch', 96), ('soon', 2), ('decide', 1), ('move', 4), ('ti', 1), ('when', 98), ('there', 92), ('2', 183), ('ones', 3), ('playing', 3), ('hand', 2), ('over', 11), ('hedge', 4), ('funny', 4), ('hooked', 1), ('days', 5), ('back', 7), ('melbourne', 1), ('hahash', 1), ('tooo', 1), ('thats', 82), ('given', 3), ('tom', 104), ('hanks', 2), ('kick', 4), ('ass', 32), ('honestly', 1), ('have', 130), ('second', 12), ('series', 177), ('sooo', 2), ('luck', 4), ('ur', 1), ('tennis', 1), ('tournament', 1), ('hating', 2), ('america', 1), ('almost', 6), ('supporting', 1), ('luv', 82), (\"book'da\", 1), ('finshed', 1), ('cannot', 1), ('begin', 81), ('comprehend', 1), ('explain', 1), ('awesomeness', 2), ('hardcore', 2), ('power', 1), ('ps', 2), ('worth', 83), ('wanted', 82), ('desperately', 80), (\"love'the\", 80), ('film', 97), ('lubb', 80), ('friday', 87), ('stayed', 3), ('watched', 17), ('mission', 1093), ('impossible', 1093), ('exciting', 1), ('ok', 163), ('update', 3), ('wow', 1), (\"haven't\", 7), ('updated', 1), ('long', 6), ('hammy', 1), ('rocks', 85), ('action', 9), ('theme', 9), ('played', 4), ('fireworks', 2), ('off', 11), ('‘', 2), ('’', 87), ('accompaniment', 2), ('whistles', 2), ('bangs', 2), ('colourfully', 2), ('sky', 2), ('unpredictable', 1), ('spy', 1), ('sort', 3), ('scenario', 1), ('astonishingly', 1), ('fat', 3), ('free', 1), ('rent', 3), ('generation', 1), ('team', 1), ('can', 107), ('infiltrate', 1), ('these', 87), ('events', 2), ('hour', 1), ('before', 9), ('doors', 1), ('officially', 5), ('finally', 4), ('feel', 7), ('making', 88), ('drive', 2), ('haunt', 2), ('tonight', 5), ('iii', 41), ('going', 96), ('year', 6), ('your', 9), ('yea', 2), ('wicked', 1), ('fall', 2), ('10', 3), ('9', 1), ('gym', 1), ('class', 2), ('looking', 3), ('suspenseful', 1), ('little', 7), ('story', 171), ('plus', 1), ('cruise', 18), ('trailers', 2), ('attached', 1), ('catch', 2), ('sure', 7), ('since', 12), ('nothing', 3), (\"i'll\", 10), ('clarksville', 1), ('summer', 1), ('big', 98), ('fan', 6), ('grown', 1), ('teevee', 1), ('flick', 2), ('cant', 1), ('saturday', 2), ('eek', 1), ('fell', 4), ('rolled', 1), ('fallon', 1), ('dad', 2), ('us', 11), ('convo', 1), ('daddy', 1), ('speaker', 1), ('exelent', 1), ('job', 2), ('ten', 3), ('ticket', 2), ('game', 1), ('jamie', 1), ('she', 169), ('had', 24), ('8', 1), ('lil', 1), ('bro', 1), ('tied', 1), ('7', 2), ('mall', 1), ('shopping', 1), ('guy', 163), ('wacked', 1), ('however', 1), ('top', 3), ('gun', 3), ('matters', 1), ('fact', 6), ('doing', 5), ('run', 2), ('pressing', 1), ('alarm', 1), ('codes', 1), ('fortress', 1), ('lol', 13), ('sing', 2), ('song', 5), ('excersizing', 1), ('makes', 2), ('funner', 1), ('cute', 2), ('latest', 1), ('kicked', 1), ('period', 1), ('characterization', 1), ('effects', 1), ('problem', 2), ('stop', 3), ('pretending', 1), ('moives', 1), ('mindless', 1), ('geek', 2), ('tech', 1), ('small', 1), ('cars', 2), ('etc', 1), ('freaking', 6), ('tickets', 2), ('turned', 81), ('definitely', 2), ('surprisingly', 1), ('cuz', 3), ('asshole', 1), ('sunday', 1), ('anyways', 3), ('instead', 3), ('least', 5), ('tests', 1), ('afterschool', 1), ('relaxed', 1), ('while', 8), ('ate', 1), ('margaritas', 1), ('w', 1), ('emily', 1), ('despise', 1), ('his', 182), ('scientology', 1), ('crusade', 1), ('btw', 3), ('stinkin', 1), ('ike', 1), ('why', 178), ('ask', 3), ('stories', 5), ('holy', 1), ('fyi', 1), ('brazil', 1), ('meeting', 2), ('theater', 5), ('harrison', 1), ('said', 92), ('viewings', 1), (\"vic's\", 1), ('bday', 1), ('party', 2), ('friends', 86), ('wondering', 3), ('been', 10), ('special', 2), ('screening', 1), ('ya', 2), ('awesomely', 1), ('um', 1), ('except', 3), ('rather', 3), ('trece', 1), ('definately', 2), ('show', 5), ('no', 14), ('bet', 1), ('blows', 1), ('exception', 1), ('eh', 1), ('cool', 86), ('experience', 1), ('camp', 1), ('setting', 1), ('land', 1), ('places', 1), ('hide', 2), ('feeling', 2), ('spin', 1), ('middle', 1), ('tan', 1), ('baby', 1), ('possum', 1), ('hot', 2), ('let', 2), ('down', 322), ('third', 2), ('especially', 3), ('scene', 2), ('vault', 1), ('chris', 2), ('kelse', 1), ('spontaneously', 1), ('10pm', 1), ('showing', 2), ('pictures', 2), (\"erin's\", 1), ('guys', 3), ('firstly', 1), ('school', 6), ('between', 3), ('classes', 4), ('settin', 1), ('fire', 5), ('halls', 1), ('partyin', 1), ('weekends', 1), ('clubbin', 1), ('lunch', 1), ('causing', 1), ('disruption', 1), ('wherever', 1), ('films', 4), ('bad', 8), ('them', 8), ('putting', 2), ('money', 5), ('pocket', 2), ('dart', 1), ('around', 85), ('considered', 1), ('hooker', 1), ('scientologist', 1), ('hero', 2), ('orig', 1), ('tv', 2), ('shows', 3), ('new', 11), ('alright', 1), ('although', 2), ('watching', 89), ('halle', 1), ('barry', 1), ('hugh', 1), ('jackson', 1), ('tc', 2), ('either', 83), ('yes', 5), ('jessica', 1), ('dumb', 3), ('frenzied', 1), ('phone', 1), ('calls', 1), ('laura', 1), ('garrett', 1), ('care', 82), (\"cruise's\", 1), ('issues', 2), ('real', 9), ('life', 8), ('play', 2), ('abrams', 1), ('knows', 83), ('reason', 1), ('explosions', 1), ('soo', 81), ('straight', 1), ('row', 1), ('magic', 1), ('today', 2), ('rocked', 1), ('preview', 1), ('used', 5), ('note', 1), ('davinci', 1), ('quip', 1), ('v', 1), ('made', 13), ('joke', 2), ('never', 88), ('freak', 1), ('pull', 1), ('style', 1), ('freakin', 82), ('th', 1), ('bond', 1), ('girls', 2), ('kicking', 1), ('stuff', 3), ('maybe', 2), ('throw', 1), ('jail', 1), ('break', 1), ('minutes', 3), (\"we're\", 81), ('gonna', 81), ('or', 263), ('hoot', 80), ('silent', 80), ('hill', 81), ('reality', 81), ('coz', 80), ('hella', 80), ('goin', 80), ('bitch', 81), ('sentry', 80), ('station', 80), ('bonkers', 80), (\"who's\", 80), (\"felicia's\", 80), ('cleaning', 80), ('table', 81), ('felicia', 80), ('grabs', 80), ('keys', 80), ('dash', 80), ('kirsten', 80), ('leah', 80), ('kate', 80), ('escapades', 80), ('politics', 2), ('harry', 2089), ('potter', 2091), ('example', 3), ('reader', 1), ('house', 2), ('excited', 1), ('proud', 2), ('goblet', 4), ('lines', 1), ('ron', 2), ('favourite', 2), ('character', 82), ('whom', 1), ('�', 3), ('eragon', 2), ('shadeslayer', 1), ('truth', 1), ('always', 86), ('thinking', 7), ('re', 2), ('6th', 3), ('blood', 2), ('prince', 2), ('final', 2), ('comes', 4), ('shoes', 2), ('picture', 6), (\"philosopher's\", 1), ('stone', 3), ('j', 2), ('k', 2), ('rowling', 2), ('strangely', 2), ('hp', 2), ('fanfic', 2), ('wish', 4), ('could', 9), ('write', 3), ('paper', 1), ('entitled', 1), ('twilight', 1), ('unfortunate', 1), ('tons', 1), ('clickfive', 1), ('kid', 2), ('loves', 3), ('cake', 1), (\"children's\", 3), ('texts', 1), ('fantasy', 2), ('perhaps', 2), ('obviously', 1), ('often', 2), ('criticized', 1), ('oversimplifying', 1), ('struggle', 1), ('vs', 2), ('evil', 32), ('might', 6), ('decent', 2), ('enough', 2), ('m', 3), ('glad', 7), ('ve', 1), ('kids', 6), ('thick', 1), ('burnt', 1), ('heavy', 1), ('enjoying', 1), ('suppose', 2), ('sawyer', 1), ('hookup', 1), ('rereading', 1), ('pop', 3), ('culture', 5), ('picky', 1), ('1st', 1), ('2nd', 1), ('clearly', 1), ('best', 15), ('funniest', 2), ('planned', 1), ('biggie', 1), ('admiring', 1), ('sisters', 1), ('collection', 1), ('waited', 1), ('costumes', 2), ('planning', 1), ('2007', 1), (\"aren't\", 1), ('unfortunately', 1), ('japenese', 1), ('food', 4), ('undoubtedly', 1), ('deal', 3), ('purchase', 1), ('la', 2), ('dvd', 1), ('narnia', 2), ('disney', 2), ('walks', 1), ('beach', 1), ('eating', 2), ('oreos', 1), ('ootp', 1), ('sixth', 1), ('desperate', 1), ('discovered', 1), ('fanfiction', 3), ('fault', 2), ('dork', 1), ('found', 1), ('german', 1), ('primary', 1), ('bits', 1), ('happiness', 1), ('usually', 1), ('form', 2), ('storytimes', 1), ('club', 4), ('occasional', 1), ('talks', 1), ('julia', 1), ('marisa', 1), ('empty', 2), ('spec', 1), ('candy', 1), ('requim', 1), ('dream', 2), ('body', 2), ('christopher', 1), ('creed', 1), ('shraddha', 1), ('lotr', 1), ('superman', 1), ('any', 3), ('budget', 1), ('nerd', 2), ('rare', 1), ('bootlegged', 1), ('daniel', 81), ('radcliffe', 1), ('ruined', 2), (\"there's\", 85), ('4', 3), ('those', 10), ('completely', 1), ('poem', 1), ('chance', 1), ('win', 4), ('fabulous', 2), ('prize', 1), ('anything', 7), ('scar', 2), ('okay', 6), ('equus', 1), ('measure', 1), ('bought', 2), ('march', 1), ('31st', 1), ('artemis', 1), ('fowl', 1), ('colony', 1), ('eoin', 1), ('colfer', 1), ('–', 2), ('385', 1), ('young', 1), ('adult', 2), ('near', 1), ('realize', 1), ('gorgeous', 3), ('adorable', 2), ('everything', 2), ('mocking', 1), (\"freagin'love\", 1), ('magical', 1), ('lore', 1), (\"y'all\", 2), ('author', 2), ('world', 7), ('personaly', 1), ('encourage', 1), ('wholesome', 1), ('fuck', 4), ('sakes', 1), ('type', 81), ('scarf', 1), ('christmas', 3), ('friend', 3), (\"she's\", 2), ('student', 2), ('kept', 1), ('saying', 6), ('g', 1), ('lilo', 1), ('stitch', 1), ('barnyard', 1), ('sexy', 4), ('part', 4), ('community', 82), ('discuss', 1), ('theories', 1), (\"sorcerer's\", 2), ('forgotten', 2), ('low', 1), ('level', 3), ('discussing', 1), ('learn', 1), ('appeal', 1), (\"eragon's\", 1), ('case', 3), ('dragons', 2), ('someone', 4), ('him', 84), ('date', 1), ('jamaica', 1), ('queens', 1), ('earrings', 1), ('goes', 1), ('adore', 1), ('hogwarts', 2), ('thanks', 1), ('comment', 3), ('33', 2), ('emma', 2), ('watson', 2), ('task', 1), ('achieved', 1), ('gotta', 4), ('icons', 2), ('main', 82), ('reasons', 1), ('fandom', 6), ('felt', 3), ('place', 84), ('belong', 1), (\"grey's\", 1), ('anatomy', 1), ('try', 8), ('lord', 5), ('rings', 5), ('none', 3), ('dedicated', 1), ('feast', 1), ('spoke', 1), ('mrs', 2), ('cowan', 1), ('briefly', 1), ('independent', 1), ('study', 1), ('incredibly', 4), ('lazy', 1), ('take', 84), ('semester', 1), ('college', 3), ('dorks', 1), ('delicious', 1), ('sucked', 600), ('away', 1), ('person', 86), ('enjoys', 1), ('depth', 1), ('conversations', 1), ('subjects', 1), ('interested', 1), ('intrigued', 1), ('death', 3), ('robe', 1), ('lends', 1), ('itself', 1), ('nicely', 1), ('halloween', 3), ('brilliant', 2), ('deemed', 1), ('gavin', 1), ('degraw', 1), ('beatles', 1), ('drawing', 1), ('need', 3), ('every', 6), ('once', 4), ('hall', 1), ('kaka', 1), ('television', 1), ('screens', 1), ('popular', 2), ('craze', 2), ('perfect', 1), ('literary', 1), (\"version's\", 1), ('adaptation', 1), ('talk', 9), ('grips', 1), ('state', 1), ('dress', 1), ('find', 1), ('anywhere', 1), ('town', 1), (\"potter's\", 2), ('veil', 1), ('darkness', 1), ('oceans', 1), ('phoenix', 1), ('word', 2), ('total', 1), ('517', 1), ('648', 1), ('counting', 1), ('fic', 2), ('fairly', 1), ('drawn', 1), ('vampire', 2), ('n', 2), ('gift', 3), ('sivullinen', 2), ('requested', 2), ('”', 5), ('nc', 3), ('17', 3), ('het', 2), ('taking', 3), ('quizzes', 2), ('deciding', 1), ('7th', 1), ('xd', 1), ('anime', 1), ('manga', 1), ('sorry', 3), ('grow', 1), ('rehearsal', 1), ('decided', 4), ('invisibility', 1), ('cloak', 1), ('folows', 1), ('man', 89), ('machine', 1), ('lets', 1), ('future', 2), ('portuguese', 1), ('spanish', 1), ('told', 6), ('200', 1), ('explaination', 1), ('travel', 1), ('packs', 1), ('animated', 1), ('emotes', 1), ('keep', 1), ('gettting', 1), ('fits', 1), ('till', 2), ('5', 81), ('apparently', 5), ('cast', 1), ('spells', 1), ('fair', 1), ('simply', 2), ('lower', 1), ('shade', 1), ('others', 1), ('posts', 1), ('anyway', 84), ('pirates', 1), ('caribbean', 1), ('writing', 2), ('sarcastic', 1), ('quirky', 1), ('sense', 2), ('humor', 1), ('otp', 1), ('shipping', 1), ('conquering', 2), ('friendships', 2), ('formed', 2), ('along', 3), ('dies', 81), ('related', 2), ('during', 3), ('winter', 1), ('specifically', 1), ('tree', 1), ('snowing', 1), ('outside', 1), ('devastate', 1), ('included', 1), ('coloured', 1), ('intellectual', 1), ('debates', 1), ('irrespective', 1), ('whether', 2), ('writes', 1), ('legacy', 1), ('hands', 3), ('hope', 4), ('grand', 1), ('finale', 1), ('lives', 1), ('hype', 1), ('younger', 1), (\"sis's\", 1), ('theres', 1), ('avatar', 1), ('sports', 1), ('messiah', 1), ('complex', 1), ('stopped', 2), ('police', 1), ('longer', 1), ('months', 2), ('through', 5), ('intense', 1), ('phase', 1), ('idk', 2), ('literature', 1), ('joining', 80), ('start', 161), ('deep', 80), ('profound', 80), ('says', 85), ('differently', 80), ('here', 163), ('serious', 80), ('count', 80), ('catcher', 80), ('tye', 80), ('jane', 80), ('eyre', 80), ('virgin', 80), ('suicides', 80), ('cowboy', 81), ('honor', 2), (\"val's\", 1), ('starring', 1), ('performance', 1), ('titus', 1), ('brokeback', 1999), ('mountain', 1999), ('cry', 2), ('asking', 2), ('rep', 1), ('cowboys', 6), ('short', 3), ('explains', 1), ('oddly', 1), ('does', 86), ('cover', 1), ('chronological', 1), ('ground', 1), ('unexpected', 1), ('crash', 10), ('basically', 1), ('diversity', 1), ('shout', 1), ('thank', 3), ('esther', 1), ('calling', 1), ('screenplay', 1), ('mph', 1), ('sale', 1), ('expo', 1), ('gosh', 1), ('news', 5), ('whereas', 1), ('brother', 1), ('cousins', 1), ('parents', 1), ('twice', 1), ('kinda', 15), ('genres', 1), ('queer', 2), ('eye', 1), ('gn', 1), ('gl', 1), ('important', 1), ('hyped', 1), ('sweeping', 1), ('moving', 1), ('bbm', 1), ('jake', 3), ('heath', 2), ('rps', 1), ('figured', 1), ('interest', 1), (\"dick's\", 1), (\"daniel's\", 1), ('hat', 81), ('picturesque', 1), ('quaintly', 1), ('attractive', 1), ('impressive', 1), ('yesterday', 2), ('asian', 2), ('tourist', 1), ('horses', 1), ('hugged', 1), ('wranglers', 1), (\"him'i\", 1), ('o', 1), ('heard', 97), ('copy', 2), ('riding', 1), ('giants', 1), ('napoleon', 1), ('dynamite', 1), ('silly', 1), ('pc', 1), ('choice', 2), ('kiss', 2), ('unbelievably', 2), ('check', 1), ('spectacularly', 1), ('confess', 1), ('judgement', 1), ('16', 1), ('£', 1), ('©', 2), (\"ain't\", 1), ('snuck', 81), (\"springer's\", 1), ('supper', 1), ('asleep', 2), ('latter', 1), ('tiny', 1), ('pink', 1), ('mentioned', 2), ('shame', 1), ('capote', 2), ('transamerica', 1), ('mention', 2), ('apart', 1), ('ang', 2), ('lee', 2), ('defensive', 1), ('positions', 1), ('gays', 1), ('wonderful', 1), ('u', 2), ('discussed', 1), ('abortion', 1), ('ban', 3), ('south', 1), ('dakota', 1), ('strip', 1), ('understand', 2), ('sexual', 1), ('industry', 2), ('tells', 1), ('won', 5), ('critics', 3), ('awards', 81), ('easy', 1), ('wondered', 1), ('happened', 1), ('ennis', 1), ('spend', 1), ('rest', 2), ('jack', 3), ('acting', 2), ('deserved', 2), ('gayer', 1), ('picnic', 1), ('basket', 1), ('haunted', 1), ('ignore', 1), ('photography', 1), ('madly', 1), ('p', 1), ('change', 1), ('background', 1), ('tragically', 1), ('romantic', 1), ('heartbraking', 1), ('dearly', 1), ('truly', 2), ('ashamed', 1), ('wept', 1), ('full', 2), ('five', 1), ('afterwards', 1), ('cried', 3), ('carefully', 2), ('done', 1), ('success', 1), ('derek', 1), ('heartbreaking', 1), (\"mountain'is\", 1), ('matter', 1), ('oscar', 4), ('gyllenhaal', 1), ('leder', 1), ('yip', 1), ('happy', 2), ('generally', 2), ('score', 3), ('changes', 1), ('receive', 1), ('give', 5), ('director', 1), ('finish', 1), ('random', 1), ('thoughts', 1), ('bitter', 1), ('bound', 1), ('white', 2), ('racism', 3), ('wins', 1), ('acceptable', 81), ('mainstream', 2), ('challenge', 2), (\"culture's\", 1), ('heteronormativity', 1), ('dissapointed', 1), ('surprised', 1), ('actor', 2), ('packed', 1), ('sceneries', 1), (\"he's\", 81), (\"'yeah\", 80), ('acne', 80), ('dudeee', 80), ('homosexuality', 80), ('becoming', 81), ('terrible', 174), ('super', 2), ('shitty', 4), ('700', 1), ('million', 1), ('tomorrow', 1), ('sucks', 601), ('boring', 92), ('god', 9), ('yahoo', 1), ('games', 1), ('awful', 17), ('themed', 1), ('skin', 1), ('chessboard', 1), ('suck', 276), ('bogus', 5), ('inaccurate', 5), ('disappointing', 1), ('written', 2), ('fabricated', 1), ('christianity', 1), ('worthless', 3), ('bolsters', 1), ('arguments', 1), ('england', 1), ('slow', 1), ('expected', 2), ('threw', 1), ('phenomenon', 1), ('question', 2), ('audrey', 3), ('tautou', 3), ('congrats', 1), ('beating', 1), ('jay', 1), ('rickards', 1), ('beat', 2), ('finals', 1), ('poorly', 1), ('hated', 38), ('balls', 16), ('worst', 1), ('piece', 2), ('rachel', 1), ('nans', 1), ('libarian', 1), (\"hated'the\", 1), ('turner', 1), ('forget', 1), ('paul', 1), ('bentlys', 1), ('butt', 2), ('timings', 1), ('ended', 1), ('fer', 1), ('runaway', 1), ('vacation', 1), ('conversation', 1), ('professors', 1), ('work', 3), ('erm', 3), ('overslept', 1), ('jenn', 1), ('later', 1), ('ballz', 1), ('loathe', 2), ('amã', 1), ('lie', 2), ('favorite', 2), ('looked', 3), ('forgot', 1), ('nd', 1), ('x3', 1), ('nacho', 1), ('libre', 1), ('t', 3), ('hank', 1), ('smoking', 1), ('moments', 1), ('everybody', 1), ('stupid', 365), ('bored', 1), ('hoover', 2), ('coherent', 1), ('uh', 1), ('less', 2), ('usage', 1), ('phrase', 1), ('dogfucking', 1), ('retarded', 1), ('worse', 1), ('started', 1), ('gathered', 1), ('lamest', 1), ('cinema', 1), (\"lama's\", 1), ('aimee', 1), ('sean', 1), ('supportive', 1), ('indian', 1), ('reservations', 1), ('lately', 1), ('hanging', 1), ('0', 117), ('hopefully', 2), ('happen', 1), ('coming', 2), ('sometime', 1), ('holding', 1), ('signs', 1), ('christ', 1), ('project', 1), ('witha', 1), ('passion', 3), ('blasphying', 1), ('name', 2), ('wide', 1), ('drove', 1), ('bayers', 1), ('lake', 1), ('dramatic', 1), ('donkey', 3), ('supposed', 1), ('due', 2), ('weekend', 3), ('stinks', 4), ('hello', 1), ('couple', 2), ('cos', 1), ('effort', 1), ('mang', 1), ('latin', 1), ('majorly', 4), ('major', 4), ('didnt', 2), ('compared', 2), ('anyhow', 1), ('rofls', 1), ('howard', 1), ('use', 2), ('google', 1), ('urls', 1), ('songs', 1), ('lucky', 1), ('refusing', 1), ('public', 1), ('reaction', 2), ('royally', 2), ('imo', 1), (\"wasn't\", 4), ('mood', 1), ('shop', 1), ('quick', 1), ('smoked', 1), ('consumed', 1), ('meat', 1), ('broke', 1), ('ideas', 1), ('sucky', 1), ('attempt', 2), ('crack', 1), ('motherfuckers', 1), ('tonite', 1), ('poseidon', 1), ('hooray', 1), ('memoirs', 1), ('geisha', 1), ('kelsey', 1), ('oceanwalk', 1), ('johnny', 2), ('rockets', 1), ('bless', 1), ('linked', 1), ('article', 2), ('hates', 82), ('review', 82), ('draw', 2), ('conclusion', 3), (\"isn't\", 3), ('true', 3), ('symantec', 1), ('suing', 1), ('microsoft', 1), ('street', 1), ('hear', 5), ('without', 1), ('touching', 1), ('christian', 2), ('delayed', 1), ('banning', 1), ('beans', 2), ('omen', 1), ('muahahaahahah', 1), ('weeeellllllll', 1), ('therefor', 1), ('frakking', 1), ('media', 1), ('seems', 1), ('thirdly', 1), ('post', 1), (\"won't\", 2), ('add', 1), ('save', 1), ('hung', 80), ('kelsie', 80), ('combining', 80), ('gary', 80), ('gin', 80), ('zen', 80), ('b', 82), ('letting', 80), ('stars', 80), ('showcasing', 1), ('aniwae', 1), (\"mission's\", 1), ('trip', 1), ('007', 1), ('getting', 2), ('freshman', 1), ('itz', 1), ('altogether', 1), ('theatan', 1), ('energy', 1), ('field', 1), ('tho', 2), ('eat', 1), ('chinese', 2), ('dim', 1), ('sum', 1), ('nice', 2), ('whos', 1), ('extent', 1), ('rv', 1), ('mi3', 2), ('lame', 4), ('predictable', 1), ('weird', 1), ('tomkat', 1), ('whole', 3), ('jill', 1), ('staying', 1), ('d', 1), ('loathed', 1), ('involving', 1), ('please', 2), ('badness', 1), ('increasing', 1), ('exponentially', 1), ('imagine', 1), ('sold', 1), ('flat', 1), ('shipmates', 1), ('tired', 2), ('franchise', 2), ('burbank', 1), ('calif', 1), ('ap', 1), ('brooke', 1), ('shields', 1), ('publicly', 1), ('apologized', 1), ('career', 1), ('including', 1), ('war', 1), ('worlds', 1), ('atrocious', 1), ('ran', 2), ('cocktail', 2), ('hahaha', 1), ('search', 1), ('boycotting', 1), ('mybutthole', 1), ('yuh', 1), ('nasy', 1), ('c', 1), ('joiners', 1), ('9am', 1), ('crappy', 6), ('aside', 1), ('phillip', 2), ('seymore', 1), ('hoffman', 2), ('nifty', 1), ('gadgets', 1), ('wif', 1), ('haha', 1), ('credit', 1), ('insane', 1), ('listen', 1), ('kanye', 1), (\"west's\", 1), ('remix', 2), ('“', 3), ('john', 4), ('woo', 4), ('laid', 2), ('egg', 2), ('sequels', 1), ('facing', 1), ('richard', 1), ('simmons', 1), ('horrible', 181), ('cringe', 2), ('whenever', 2), ('respect', 2), ('closet', 1), ('blogbacklinktitle', 1), ('blogbacklinksnippet', 1), ('seymour', 1), ('simon', 1), ('pegg', 1), ('prediction', 1), ('correct', 1), ('lousy', 2), ('boycott', 1), ('point', 2), ('talkin', 1), ('bout', 1), ('mi', 1), ('agreed', 1), ('fit', 2), ('cobequid', 1), ('health', 1), ('center', 1), ('soooooooo', 1), ('aka', 2), ('et', 1), ('al', 1), ('useless', 1), ('realized', 1), (\"they'd\", 1), ('actual', 2), ('blame', 2), ('chunnel', 1), ('fears', 1), ('scared', 1), ('regardless', 1), ('track', 1), ('dumbest', 1), ('futile', 1), ('ending', 2), ('began', 1), ('teri', 1), ('12', 1), ('interview', 1), ('anne', 1), ('rice', 1), ('attraction', 1), ('reopened', 1), ('release', 1), ('unable', 1), ('ignorant', 1), ('villains', 1), ('creatures', 1), ('called', 1), ('dementors', 2), ('bullshit', 3), ('costume', 2), ('monthly', 1), ('mad', 1), ('vintage', 1), ('silver', 1), ('tea', 2), ('infuser', 2), ('shaped', 1), ('frog', 1), ('die', 2), ('depressing', 188), ('idiot', 1), ('sometimes', 2), ('wesley', 2), ('captain', 2), ('picard', 2), ('douche', 2), ('kenley', 1), ('decides', 1), ('plastic', 1), ('axes', 1), ('amazes', 1), ('afraid', 1), ('children', 1), ('tragic', 2), ('pastings', 1), ('mpreg', 1), ('archive', 1), ('section', 1), ('starting', 1), ('rant', 3), ('deluded', 1), ('loose', 1), ('allegory', 1), ('bible', 2), ('mother', 3), ('georgia', 4), ('wants', 1), ('her', 4), ('local', 1), ('board', 4), ('schools', 1), ('libraries', 2), ('leads', 1), ('witchcraft', 2), ('according', 1), ('images', 1), ('lin', 1), ('pale', 1), ('comparrison', 1), ('overlooking', 1), ('color', 2), ('hair', 1), ('trivia', 1), ('eyes', 1), ('absolute', 1), ('denial', 1), ('general', 1), ('involved', 1), ('dumbass', 1), ('highly', 1), ('inappropriate', 1), (\"headmistress's\", 1), ('office', 1), ('melandry', 1), (\"let's\", 1), ('fuckers', 1), ('shut', 1), ('dance', 2), ('ripper', 1), ('following', 1), ('clips', 1), ('featured', 1), ('obnoxious', 2), ('believably', 1), ('judging', 1), ('generated', 1), ('gives', 1), ('returning', 1), ('thinks', 2), ('past', 81), ('spite', 1), ('knowing', 1), ('clit', 1), (\"was'harry\", 1), ('everytime', 1), (\"said'harry\", 1), (\"sucks'i\", 1), ('outnumbered', 1), ('thousand', 2), (\"saying'harry\", 1), ('rules', 1), ('ultimate', 2), ('marvel', 2), ('anax', 1), ('rife', 1), ('faked', 1), ('mary', 1), ('sue', 1), ('ism', 1), ('hogwash', 1), ('education', 2), ('listens', 1), (\"mother's\", 2), ('campaign', 1), ('rid', 1), ('nearly', 4), ('exquisite', 2), ('sucking', 6), ('cock', 85), ('soooooo', 1), ('claiming', 1), ('drain', 1), ('warns', 1), ('indoctrinate', 1), ('wicca', 1), ('seeking', 1), ('wiccans', 1), ('react', 1), ('possible', 1), ('marcia', 2), ('gaither', 2), ('teaches', 2), ('wiccanism', 2), ('controversy', 2), ('look', 1), ('precious', 1), ('classic', 1), ('cheapened', 1), ('association', 1), ('glitz', 1), ('hollywood', 4), ('gossip', 1), ('talked', 1), ('writer', 1), ('immortal', 1), ('working', 2), ('psychology', 1), ('thesis', 1), ('iq', 1), ('cucumber', 1), ('potterholic', 1), ('ew', 1), ('jame', 1), ('living', 1), ('guts', 1), ('depp', 1), ('needs', 81), ('haircut', 1), ('meganpenworthy', 1), ('dressed', 1), ('selfish', 1), ('iron', 1), ('optimus', 1), ('prime', 1), ('1984', 1), ('indicative', 1), ('overall', 2), ('decline', 1), ('english', 1), ('speaking', 3), ('predictability', 1), ('packaging', 1), ('standpoint', 1), ('dont', 3), ('horridly', 1), ('basic', 1), ('cheap', 1), ('christain', 1), ('lynne', 1), ('devil', 1), ('chronicles', 1), ('lords', 2), ('ring', 1), ('arse', 1), ('rides', 1), ('broom', 1), ('simple', 1), ('exhausted', 1), ('hilarious', 1), ('fiber', 1), ('inherently', 1), ('invisible', 1), ('murdered', 1), ('fade', 1), ('creature', 1), ('demeantor', 1), ('soul', 2), ('mouth', 3), ('decaying', 1), ('decomposing', 1), ('hippie', 1), ('bringing', 1), ('mcgarther', 1), ('park', 1), ('chamber', 1), ('secrets', 1), ('prisoner', 2), ('azkaban', 2), ('equally', 2), ('giving', 1), ('benefit', 1), ('doubt', 1), ('online', 1), ('offence', 1), ('roommate', 1), ('believe', 1), ('facebook', 1), ('group', 1), (\"called'harry\", 1), ('addition', 1), ('screwed', 1), ('malaguena', 1), ('russotti', 1), ('changed', 1), ('plays', 1), ('dumbledor', 1), ('hours', 1), ('ago', 2), ('suncoast', 1), ('kat', 1), ('tun', 1), ('starred', 1), ('opened', 2), ('dictate', 2), ('reply', 2), ('cut', 2), ('mirror', 2), ('topic', 1), ('yuck', 1), ('gladly', 1), ('dungeons', 1), ('asks', 1), ('scent', 1), ('nature', 1), ('hairy', 1), ('otters', 1), ('wussies', 1), ('joy', 1), ('live', 3), ('titanic', 1), ('durno', 1), ('offense', 1), ('whatev', 1), ('huge', 1), ('loser', 1), ('hahahaha', 1), ('sent', 1), ('home', 2), (\"50's\", 1), ('musiclove', 1), ('crystal', 1), ('condeming', 1), ('walk', 1), ('forever', 1), ('shell', 1), ('huh', 1), ('jelly', 1), ('pic', 1), (\"frodo's\", 1), ('vice', 1), ('versa', 1), ('whiny', 1), ('goth', 2), ('canceled', 1), ('postponed', 1), ('hermione', 1), ('childishly', 1), ('quiz', 80), ('bye', 80), ('retarted', 80), ('black', 80), ('outshines', 80), ('material', 80), ('plain', 80), ('likes', 80), ('dragged', 80), ('draco', 80), ('malfoy', 80), ('trousers', 80), ('hips', 80), ('throat', 80), ('vigor', 80), ('whimpering', 80), ('noises', 80), ('panting', 80), ('groaning', 80), ('blonds', 80), ('rock', 80), ('aching', 80), ('wotshisface', 80), ('slap', 80), ('immensely', 1), ('executed', 1), ('moralistic', 1), ('…', 2), ('actors', 1), ('awkward', 1), ('academy', 2), ('award', 1), ('harder', 1), ('pup', 1), ('tent', 1), (\"everybody's\", 1), ('boys', 1), ('fill', 1), ('blanks', 1), ('joan', 1), ('keeps', 1), ('characters', 1), ('juicy', 1), ('posted', 1), ('livejournal', 1), ('dick', 1), ('µª', 1), (\"luck's\", 1), ('close', 1), ('gayness', 2), ('waaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 1), ('anus', 1), ('larry', 1), ('mcmurtry', 1), ('diana', 1), ('ossana', 1), ('freezing', 1), ('evilpinkmunky', 1), ('soooo', 1), ('shameful', 1), ('master', 1), ('himself', 1), ('sitting', 1), ('head', 81), ('chicken', 1), ('sam', 1), ('scenes', 1), ('backdrop', 1), ('tried', 2), ('ah', 1), ('nothin', 1), ('heather', 1), ('shes', 1), ('stites', 1), (\"knight's\", 1), ('tale', 2), ('seemed', 1), ('josie', 1), ('zach', 1), ('loudest', 1), ('criticizers', 1), ('didnâ', 1), ('\\x80', 2), ('\\x99', 2), ('donâ', 1), ('nanny', 1), ('mcphee', 1), ('revise', 1), ('jokes', 81), ('unless', 1), ('crafted', 1), ('involve', 1), ('00', 1), ('rode', 1), ('bikes', 1), ('rented', 2), ('complaints', 1), ('vito', 1), ('plot', 1), ('twist', 2), ('facile', 1), ('comparsions', 1), ('overcoming', 1), ('adversity', 1), ('stupidest', 1), ('offensive', 1), ('concocted', 1), ('dinner', 1), ('min', 1), ('rosie', 1), ('record', 2), ('independant', 1), ('pudding', 1), ('category', 1), ('shattered', 1), ('spine', 1), ('emo', 1), ('mound', 1), ('cow', 1), ('kill', 1), ('writers', 1), ('ruining', 1), ('image', 1), ('liberal', 1), ('exploitation', 1), ('boyy', 1), ('hahahahahaha', 1), ('weiners', 1), ('murderball', 2), ('immediately', 2), ('robbed', 2), ('favor', 2), ('generalized', 1), ('condemnation', 1), ('entire', 1), ('voted', 1), ('targeted', 1), ('mob', 1), (\"watched'10\", 1), (\"you'and\", 1), (\"of'brokeback\", 1), ('homophobic', 1), ('dislike', 2), ('died', 1), ('france', 1), ('portugal', 1), ('match', 1), ('dvds', 1), ('kudos', 1), ('blog', 1), ('overexagerated', 1), ('requiem', 1), ('patirot', 1), ('knights', 1), ('four', 1), ('feathers', 1), ('casanova', 1), ('dogtown', 1), ('monsters', 1), ('ball', 1), ('visually', 1), ('incredible', 1), ('extremely', 3), ('directed', 1), ('lapse', 1), ('created', 1), ('account', 1), ('comments', 1), ('becuase', 1), ('consider', 1), ('shittiest', 2), ('disliked', 3), ('reference', 1), ('answers', 1), ('wrote', 1), ('letter', 1), ('editor', 1), ('6', 1), ('folk', 1), ('families', 1), ('boycotted', 1), ('festivities', 1), ('madre', 1), ('tiffani', 1), ('pirated', 1), ('illegally', 1), ('presented', 1), ('subtitles', 1), ('backward', 1), ('mormon', 1), ('danielle', 1), ('mostly', 1), ('honest', 1), ('lesson', 1), ('fix', 1), ('homophobes', 1), ('corrupting', 1), ('thursday', 1), ('hollywoord', 1), ('reviews', 1), ('fully', 1), ('clothed', 1), ('combonation', 1), ('halfway', 1), ('van', 1), ('insurance', 1), ('runs', 1), ('room', 1), ('messy', 1), ('clean', 1), ('ony', 1), ('monchel', 1), ('brigid', 1), ('helped', 80), ('bobbypin', 80), ('insanely', 80), ('laughed', 80), (\"dad's\", 80), ('sit', 80), ('mtv', 80), ('reminded', 80), ('despised', 80)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDEp83AfGKHn"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_counts), 100, input_length=sequences_same_size.shape[1]))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "fN0Sve3Pucti",
        "outputId": "992db3a6-11f0-483a-df86-77413ce700be"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 933, 100)          222100    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 256)               365568    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 587,925\n",
            "Trainable params: 587,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "RLnhNZAEuf0w",
        "outputId": "5fc6ae1c-bcba-41dd-f798-bd87094ad4fd"
      },
      "source": [
        "history = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=3, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 6225 samples, validate on 692 samples\n",
            "Epoch 1/3\n",
            "6225/6225 [==============================] - 181s 29ms/step - loss: 0.3010 - acc: 0.8859 - val_loss: 0.0574 - val_acc: 0.9798\n",
            "Epoch 2/3\n",
            "6225/6225 [==============================] - 181s 29ms/step - loss: 0.0268 - acc: 0.9920 - val_loss: 0.0450 - val_acc: 0.9899\n",
            "Epoch 3/3\n",
            "6225/6225 [==============================] - 181s 29ms/step - loss: 0.0103 - acc: 0.9974 - val_loss: 0.0223 - val_acc: 0.9928\n"
          ]
        }
      ]
    }
  ]
}