{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daeijavad/Deep-Learning/blob/main/HW6_LSTMTextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XWjy9-zWLatu",
        "outputId": "40e67b21-4fa3-44f3-c86e-51e64e6f8a13"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qauAcGDALPji"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK6n9Bg2LPjk"
      },
      "source": [
        "Welcome to HomeWork 6  &#128522;&#9996;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPJyEKTzLPjl"
      },
      "source": [
        "## Part I : Text Generation Using RNNs\n",
        "\n",
        "This part aims to generate text using machine learning models (RNN). \n",
        "\n",
        "As usual, we will start by loading the packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "DXU0_oGULPjn",
        "outputId": "e14d7c03-578a-4260-ebef-df3e99776325"
      },
      "source": [
        "import tensorflow as tf\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "np.random.seed(1)\n",
        "# Feel free to add pakages depending of your selected frame work. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT4eT7tYLPjq"
      },
      "source": [
        "Then, we load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzcEZ4l5LPjr"
      },
      "source": [
        "path_to_file = \"/content/drive/My Drive/Colab Notebooks/dataset1.txt\"\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69P2I_adLPju"
      },
      "source": [
        "Now, take a look at the first 200 characters in text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "v9FLp43zLPjv",
        "outputId": "c3636578-1d9a-447c-c2ea-c3f59bc24497"
      },
      "source": [
        "print(text[:200] + \"...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "﻿\"Well, Prince, so Genoa and Lucca are now just family estates of the\r\n",
            "Buonapartes. But I warn you, if you don't tell me that this means war,\r\n",
            "if you still try to defend the infamies and horrors perpe...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Focy0oc9LPjz"
      },
      "source": [
        "From now implement your model.\n",
        "<br>\n",
        "Please add cells and explain yours developing steps and your results.\n",
        "\n",
        "استفاده از زبان فارسی برای توضیحات هم مجاز است\n",
        "\n",
        "موفق باشید"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MowEqpNr7QLT"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ye_NYOGvLPj0",
        "outputId": "3590cef7-0d81-4339-b319-e9e6fb6be325"
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9w2ywh67WzQ"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "> in this part we first convert text in to nparray\n",
        "\n",
        "> then we make X and Y, for each X the next character is corresponding Y\n",
        "\n",
        "> then we change Y into categorical form and normalize X(because we have set input of LSTM to be tanh and it should be between -1,1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LyrVMGlOFRZ"
      },
      "source": [
        "data = np.array(list(text))\n",
        "X = data.view(np.uint8)[0:-1]\n",
        "Y = data.view(np.uint8)[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ZHqIXSC-wJ"
      },
      "source": [
        "Y = np_utils.to_categorical(Y)\n",
        "X = (X-128)/256\n",
        "X = np.reshape(X, (len(X), 1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmecA1nT7jCB"
      },
      "source": [
        "# Model \n",
        "> a LSTM followed by a Dropout layer and then after flatten layer Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "_PQR4Lq9Oa4u",
        "outputId": "d343386a-059d-4fa8-a5cb-c31a707c70b1"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256,recurrent_activation='tanh', input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "Baw-MP7F-zsX",
        "outputId": "0a7bb495-3741-4928-fb3b-16cc63fe3eb6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 1, 256)            264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 255)               65535     \n",
            "=================================================================\n",
            "Total params: 329,727\n",
            "Trainable params: 329,727\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW67t5Xb9ARp"
      },
      "source": [
        "# Train\n",
        "\n",
        "### It gets lots of time to learn(also in colab) So we do it for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "yVwaaNHX0CyR",
        "outputId": "274d8882-6165-4c65-cd62-83c6cd64e2ad"
      },
      "source": [
        "history1 = model.fit(X, Y, epochs=10, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "13032907/13032907 [==============================] - 506s 39us/step - loss: 1.2684 - acc: 0.7500\n",
            "Epoch 2/10\n",
            "13032907/13032907 [==============================] - 502s 39us/step - loss: 1.2611 - acc: 0.7500\n",
            "Epoch 3/10\n",
            "13032907/13032907 [==============================] - 501s 38us/step - loss: 1.2603 - acc: 0.7500\n",
            "Epoch 4/10\n",
            "13032907/13032907 [==============================] - 497s 38us/step - loss: 1.2598 - acc: 0.7500\n",
            "Epoch 5/10\n",
            "13032907/13032907 [==============================] - 504s 39us/step - loss: 1.2594 - acc: 0.7500\n",
            "Epoch 6/10\n",
            "12865152/13032907 [============================>.] - ETA: 6s - loss: 1.2591 - acc: 0.7500Buffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvS62ICz87_j"
      },
      "source": [
        "# predict\n",
        "> because in my model every character depends on the character before,  we should give \"she \" as input \" \"(single space)\n",
        "\n",
        "> between 3 most probable chars it selects randomly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "BGuXvxgL86zR",
        "outputId": "47aba42d-e56e-4473-d267-63088e0fbdfd"
      },
      "source": [
        "import random\n",
        "x = ' '\n",
        "string = []\n",
        "for i in range(200):\n",
        "  x = np.reshape((np.array(list(x))).view(np.uint8)[0], (1, 1, 1))\n",
        "  x = (x - 128)/256\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = random.choice(np.argsort(prediction)[:,-3:][0])\n",
        "  x = chr(index)\n",
        "  string.append(x)\n",
        "print(string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['v', 'O', 'O', '\\x00', ' ', '\"', '\"', '\"', '\"', 'v', 'O', '\\\\', '\\\\', '\\\\', '\\\\', 'O', 'O', '\\\\', '\\x00', '\\x00', 'e', '\\x00', 'e', 'O', '\\x00', '\\x00', 'e', '\\x00', '\\x00', ' ', '\\x00', 'e', 'O', '\\\\', 'O', 'O', '\\\\', '\\\\', 'O', 'O', 'O', '\\x00', '\\x00', '\\x00', '\\x00', 'e', '\\x00', 'e', '\\\\', 'O', 'O', '\\\\', '\\\\', '\\x00', 'e', '\\\\', 'O', '\\\\', '\\\\', 'O', '\\\\', 'O', 'O', '\\\\', '\\x00', '\\x00', ' ', '\\x00', ' ', '\\x00', ' ', '\\x00', ' ', 'v', 'O', 'O', '\\x00', '\\x00', '\\x00', ' ', '\\x00', '\\x00', ' ', 'v', '\\\\', '\\x00', 'e', 'O', '\\x00', 'e', 'O', '\\x00', '\\x00', 'e', '\\\\', '\\\\', 'O', '\\\\', 'O', '\\\\', '\\\\', 'O', '\\\\', 'O', '\\x00', ' ', '\\x00', 'e', '\\\\', 'O', 'O', 'O', '\\\\', '\\x00', '\\x00', '\\x00', 'e', '\\\\', '\\\\', '\\\\', '\\\\', 'O', '\\x00', ' ', '\"', '\\x00', '\\x00', ' ', '\"', 'v', '\\x00', ' ', '\"', 'v', '\\x00', 'e', '\\\\', 'O', '\\\\', '\\\\', 'O', '\\\\', 'O', 'O', 'O', '\\\\', '\\x00', ' ', 'v', '\\x00', ' ', '\"', '\\x00', ' ', '\\x00', '\\x00', '\\x00', '\\x00', 'e', '\\x00', 'e', '\\x00', '\\x00', ' ', '\\x00', 'e', '\\x00', '\\x00', 'e', 'O', 'O', '\\\\', '\\\\', '\\\\', '\\x00', 'e', '\\\\', '\\\\', 'O', '\\x00', 'e', 'O', '\\x00', 'e', '\\\\', '\\\\', '\\x00', '\\x00', ' ', 'v', '\\\\', '\\x00', ' ', '\"', '\"', '\"', '\\x00', '\\x00', '\\x00', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fde2SSggY2D9"
      },
      "source": [
        "as we can see there are always '\\\\' 'v' 'O' '\\x00'.\n",
        "in the following we show how to have a better model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiRx2ffhAaYj"
      },
      "source": [
        "# Better model\n",
        "for having better model we can do these thing:\n",
        "\n",
        "\n",
        "1.   instead of predicting from only one character befere, we can use for example 4 characters before.\n",
        "2.   it's a good idea to have better model for example 2 LSTM respectively\n",
        "3.   we can have better categorical implementation of Y. Now Y gets more than 12GB of RAM only because of lots of zero, for each character it uses one hot vector with length = 256 but there are lots of chars that has not been used once in text and are always zero :|\n",
        "4.   for better performance it's would be good if we change all text into lowercase\n",
        "\n",
        "\n",
        "## we do all above things, bellow:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s9k1Q-_fRfw"
      },
      "source": [
        "# preproccess\n",
        "> 4 char are used to predict next char"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sh4mUWcDDqz"
      },
      "source": [
        "# number 4\n",
        "text = text.lower()\n",
        "# for number 3 we use a dic for each char\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "n_vocab = len(chars)\n",
        "n_chars = len(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOjTNO9hZmuo"
      },
      "source": [
        "# number 1\n",
        "seq_length = 4\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = text[i:i + seq_length]\n",
        "\tseq_out = text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-vMIPYxZ01F"
      },
      "source": [
        "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
        "\n",
        "# this time we use sigmoid so it's between 0,1\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "#one-hot vector\n",
        "Y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbZPQM5Xet_E"
      },
      "source": [
        "# Model2 train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtVC8uGQaWPp"
      },
      "source": [
        "# number 2\n",
        "model2 = Sequential()\n",
        "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(256, return_sequences=True))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(256))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(Y.shape[1], activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "xp5Qjlr2ao5O",
        "outputId": "65f18a5c-9b1b-492f-a6c0-3757f9eb1e4c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 1, 256)            264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 255)               65535     \n",
            "=================================================================\n",
            "Total params: 329,727\n",
            "Trainable params: 329,727\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "vXzjVbT1au4S",
        "outputId": "b055d92b-bf07-4313-ea8c-ca9e46a18962"
      },
      "source": [
        "history2 = model2.fit(X, Y, epochs=10, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3258223/3258223 [==============================] - 590s 181us/step - loss: 2.1889 - acc: 0.3668\n",
            "Epoch 2/10\n",
            "3258223/3258223 [==============================] - 589s 181us/step - loss: 1.8213 - acc: 0.4599\n",
            "Epoch 3/10\n",
            "3258223/3258223 [==============================] - 583s 179us/step - loss: 1.7078 - acc: 0.4877\n",
            "Epoch 4/10\n",
            "3258223/3258223 [==============================] - 583s 179us/step - loss: 1.6443 - acc: 0.5028\n",
            "Epoch 5/10\n",
            "3258223/3258223 [==============================] - 582s 179us/step - loss: 1.6034 - acc: 0.5128\n",
            "Epoch 6/10\n",
            "3258223/3258223 [==============================] - 594s 182us/step - loss: 1.5732 - acc: 0.5202\n",
            "Epoch 7/10\n",
            " 466176/3258223 [===>..........................] - ETA: 8:23 - loss: 1.5576 - acc: 0.5235Buffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHBvFoLme1T2"
      },
      "source": [
        "# Model2 predict\n",
        "> again we choose between best 3 probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "eEe4Se3ea0xW",
        "outputId": "9b6f86bb-9d83-4c8d-b44e-ca4b94c3a888"
      },
      "source": [
        "x = 'she '\n",
        "char4 = [char_to_int[char] for char in x]\n",
        "string = []\n",
        "for i in range(200):\n",
        "  char4 = np.reshape(char4, (1, 4, 1))\n",
        "  x = char4 / float(n_vocab)\n",
        "  prediction = model2.predict(x, verbose=0)\n",
        "  index = random.choice(np.argsort(prediction)[:,-3:][0])\n",
        "  result = int_to_char[index]\n",
        "  string.append(result)\n",
        "  char4 = np.append(char4,index)[1:5]\n",
        "\n",
        "# replace \\r and \\n with space to show all text\n",
        "print(''.join(string).replace('\\r\\n', ' ').replace('\\n', ' ').replace('\\r', ' '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seen.\"\"\" her hant to stirued hilsed.\"\" said, 2)\"\"\" had or they that her.  tea. too anyot wered tilen while hav intole.\" a chiev askies. took, anontentent an i shal this, a convity..  two, a smile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS42NM8QnJoP"
      },
      "source": [
        "for really better performance we can use word embedding and word2vec"
      ]
    }
  ]
}